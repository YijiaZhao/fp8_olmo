diff --git a/examples/utils.py b/examples/utils.py
old mode 100644
new mode 100755
index 14be069d..51654b1b
--- a/examples/utils.py
+++ b/examples/utils.py
@@ -64,6 +64,8 @@ INTERNLM_META_INSTRUCTION = """You are an AI assistant whose name is InternLM (
 
 QWEN_PROMPT_TEMPLATE = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n"
 
+Olmo_PROMPT_TEMPLATE = "<|user|>\n{input_text}\n<|assistant|>\n"
+
 DEFAULT_PROMPT_TEMPLATES = {
     'InternLMForCausalLM': "<|User|>:{input_text}<eoh>\n<|Bot|>:",
     'InternLM2ForCausalLM': "<|im_start|>system\n" + INTERNLM_META_INSTRUCTION +
@@ -72,6 +74,7 @@ DEFAULT_PROMPT_TEMPLATES = {
     'QWenForCausalLM': QWEN_PROMPT_TEMPLATE,
     'Qwen2ForCausalLM': QWEN_PROMPT_TEMPLATE,
     'Qwen2MoeForCausalLM': QWEN_PROMPT_TEMPLATE,
+    'OlmoForCausalLM': Olmo_PROMPT_TEMPLATE
 }
 
 
diff --git a/tensorrt_llm/models/__init__.py b/tensorrt_llm/models/__init__.py
index e70ffc13..b0b85d4e 100755
--- a/tensorrt_llm/models/__init__.py
+++ b/tensorrt_llm/models/__init__.py
@@ -190,4 +190,5 @@ MODEL_MAP = {
     'RobertaModel': RobertaModel,
     'RobertaForQuestionAnswering': RobertaForQuestionAnswering,
     'RobertaForSequenceClassification': RobertaForSequenceClassification,
+    'OlmoForCausalLM':LLaMAForCausalLM,
 }
diff --git a/tensorrt_llm/models/llama/config.py b/tensorrt_llm/models/llama/config.py
old mode 100644
new mode 100755
index 10ea8a30..dc0bc07d
--- a/tensorrt_llm/models/llama/config.py
+++ b/tensorrt_llm/models/llama/config.py
@@ -132,7 +132,7 @@ class LLaMAConfig(PretrainedConfig):
             norm_epsilon = getattr(hf_config, "layer_norm_epsilon", 1e-5)
         else:
             hidden_act = hf_config.hidden_act
-            norm_epsilon = hf_config.rms_norm_eps
+            norm_epsilon = getattr(hf_config, 'rms_norm_eps', 1e-5)
         head_dim = getattr(
             hf_config, "head_dim",
             hf_config.hidden_size // hf_config.num_attention_heads)
diff --git a/tensorrt_llm/models/llama/model.py b/tensorrt_llm/models/llama/model.py
old mode 100644
new mode 100755
index 06f0ee39..45b7ec6f
--- a/tensorrt_llm/models/llama/model.py
+++ b/tensorrt_llm/models/llama/model.py
@@ -23,7 +23,7 @@ from ...functional import (AllReduceFusionOp, AllReduceParams, Tensor,
                            allgather, concat, non_gated_version, recv, send)
 from ...layers import (MOE, Attention, AttentionMaskType, ColumnLinear,
                        Embedding, FusedGatedMLP, GatedMLP,
-                       PositionEmbeddingType, RmsNorm)
+                       PositionEmbeddingType, RmsNorm, LayerNorm)
 from ...lora_manager import LoraConfig, use_lora
 from ...mapping import Mapping
 from ...module import Module
@@ -50,8 +50,9 @@ class LLaMADecoderLayer(Module):
 
         if (self.config.use_input_layernorm_in_first_layer
                 and self.layer_idx == 0) or self.layer_idx > 0:
-            self.input_layernorm = RmsNorm(normalized_shape=config.hidden_size,
-                                           eps=config.norm_epsilon,
+            self.input_layernorm = LayerNorm(normalized_shape=config.hidden_size,
+                                           eps=1e-5,
+                                           elementwise_affine=False,
                                            dtype=config.dtype)
 
         layers_range = config.mapping.pp_layers(config.num_hidden_layers)
@@ -98,9 +99,10 @@ class LLaMADecoderLayer(Module):
                           quant_mode=config.quant_mode,
                           **mlp_kwargs)
 
-        self.post_layernorm = RmsNorm(normalized_shape=config.hidden_size,
-                                      eps=config.norm_epsilon,
-                                      dtype=config.dtype)
+        self.post_layernorm = LayerNorm(normalized_shape=config.hidden_size,
+                                           eps=1e-5,
+                                           elementwise_affine=False,
+                                           dtype=config.dtype)
 
         # Residual MLP that applies on pre-attention input
         # TODO: change to self.has_residual_mlp = self.config.residual_mlp after ModelOpt quantize config is updated
@@ -173,7 +175,7 @@ class LLaMADecoderLayer(Module):
                 if default_net().plugin_config.reduce_fusion else
                 AllReduceFusionOp.NONE,
                 residual=residual,
-                norm_weight=self.post_layernorm.weight.value,
+                norm_weight=self.post_layernorm.weight.value if self.post_layernorm.weight else None,
                 scale=reduce_fusion_scale,
                 eps=self.post_layernorm.eps))
 
@@ -268,9 +270,10 @@ class LLaMAModel(Module):
         if self.mapping.is_last_pp_rank():
             self.ln_f = None
             if config.use_last_layernorm:
-                self.ln_f = RmsNorm(normalized_shape=config.hidden_size,
-                                    eps=config.norm_epsilon,
-                                    dtype=config.dtype)
+                self.ln_f = LayerNorm(normalized_shape=config.hidden_size,
+                                           eps=1e-5,
+                                           elementwise_affine=False,
+                                           dtype=config.dtype)
 
     def forward(self,
                 input_ids,
diff --git a/tensorrt_llm/quantization/quantize_by_modelopt.py b/tensorrt_llm/quantization/quantize_by_modelopt.py
index a90179fe..e3cc9859 100755
--- a/tensorrt_llm/quantization/quantize_by_modelopt.py
+++ b/tensorrt_llm/quantization/quantize_by_modelopt.py
@@ -133,6 +133,7 @@ MODEL_NAME_PATTERN_MAP = {
     "Exaone": "exaone",
     "DeciLMForCausalLM": "deci",
     "DeepseekForCausalLM": "deepseek",
+    "OlmoForCausalLM":"olmo",
 }
 
 
